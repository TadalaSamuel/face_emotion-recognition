{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T12:48:14.326739Z",
     "iopub.status.busy": "2023-09-12T12:48:14.326059Z",
     "iopub.status.idle": "2023-09-12T12:48:26.561499Z",
     "shell.execute_reply": "2023-09-12T12:48:26.559751Z",
     "shell.execute_reply.started": "2023-09-12T12:48:14.326703Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip install face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T12:48:26.564587Z",
     "iopub.status.busy": "2023-09-12T12:48:26.564166Z",
     "iopub.status.idle": "2023-09-12T12:48:38.066270Z",
     "shell.execute_reply": "2023-09-12T12:48:38.064926Z",
     "shell.execute_reply.started": "2023-09-12T12:48:26.564549Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T12:48:38.068494Z",
     "iopub.status.busy": "2023-09-12T12:48:38.067977Z",
     "iopub.status.idle": "2023-09-12T12:48:49.734105Z",
     "shell.execute_reply": "2023-09-12T12:48:49.732790Z",
     "shell.execute_reply.started": "2023-09-12T12:48:38.068456Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-12T12:48:49.737877Z",
     "iopub.status.busy": "2023-09-12T12:48:49.737517Z",
     "iopub.status.idle": "2023-09-12T12:48:51.951937Z",
     "shell.execute_reply": "2023-09-12T12:48:51.949154Z",
     "shell.execute_reply.started": "2023-09-12T12:48:49.737849Z"
    },
    "id": "bSpyoxVP96lN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 203 ms\n",
      "Wall time: 214 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms,models\n",
    "from livelossplot import PlotLossesKeras\n",
    "import matplotlib.pyplot as plt\n",
    "import dlib\n",
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "train_dir_affect=r\"C:\\Users\\mitvo\\Documents\\Facial_Image_Dataset\\archive\"\n",
    "\n",
    "predictor = dlib.shape_predictor(r\"C:\\Users\\mitvo\\Documents\\Facial_Image_Dataset\\shape_predictor_81_face_landmarks.dat\")  # You need to download this file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-12T12:48:51.977182Z",
     "iopub.status.idle": "2023-09-12T12:48:51.977681Z",
     "shell.execute_reply": "2023-09-12T12:48:51.977466Z",
     "shell.execute_reply.started": "2023-09-12T12:48:51.977442Z"
    }
   },
   "source": [
    "# Function to load images and apply face detection\n",
    "def load_labels_and_landmarks(directory):\n",
    "    captured_images_count=0\n",
    "    Non_captured_images_count=0\n",
    "\n",
    "    for expression_index, expression_dir in enumerate(os.listdir(directory)):\n",
    "        expression_path = os.path.join(directory, expression_dir)\n",
    "        if os.path.isdir(expression_path):\n",
    "            for image_file in os.listdir(expression_path):\n",
    "                image_path = os.path.join(expression_path, image_file)\n",
    "                img = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
    "                face_locations = face_recognition.face_locations(img)\n",
    "                for (x, y, w, h) in face_locations:\n",
    "                    landmarks = predictor(img,dlib.rectangle(left=x, top=y, right=w, bottom=h))\n",
    "                if face_locations!=():\n",
    "                    captured_images_count+=1\n",
    "                else:\n",
    "                    Non_captured_images_count+=1\n",
    "    return captured_images_count,Non_captured_images_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = r\"C:\\Users\\mitvo\\OneDrive - Maganti IT Solutions PVT LTD\\Documents\\Facial_Image_Dataset\\archive\\anger\\image0000399.jpg\"\n",
    "\n",
    "# Split the path using \"\\\\\" to separate directories and filename\n",
    "parts = path.split(\"\\\\\")\n",
    "print(\"Word behind \\\\image:\", parts[-2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "points[(15, 56), (16, 47), (17, 37), (18, 29), (23, 21), (30, 15), (39, 11), (49, 8), (58, 8), (65, 10), (69, 15), (73, 22), (75, 29), (76, 36), (77, 43), (76, 49), (74, 56), (31, 63), (35, 68), (41, 70), (48, 70), (54, 67), (64, 67), (67, 69), (71, 70), (74, 69), (75, 65), (60, 60), (61, 55), (62, 51), (64, 46), (54, 42), (58, 41), (61, 40), (65, 41), (67, 43), (38, 58), (42, 59), (45, 60), (49, 58), (46, 58), (42, 58), (64, 59), (67, 60), (70, 60), (72, 59), (70, 59), (67, 59), (42, 35), (50, 36), (57, 36), (61, 36), (64, 36), (68, 36), (71, 35), (68, 30), (64, 28), (60, 27), (56, 27), (49, 29), (43, 34), (56, 34), (60, 34), (64, 34), (69, 35), (64, 31), (60, 30), (56, 30), (29, 86), (37, 87), (48, 87), (58, 87), (66, 88), (69, 85), (74, 70), (23, 75), (28, 82), (16, 61), (73, 62), (72, 80), (65, 87)]\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import face_recognition\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the face landmark predictor\n",
    "#predictor = dlib.shape_predictor(\"/kaggle/input/shape-predictor81/shape_predictor_81_face_landmarks.dat\")  # You need to download this file\n",
    "image_path = r\"C:\\Users\\mitvo\\Documents\\Facial_Image_Dataset\\archive\\anger\\image0000399.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Apply Gaussian blur\n",
    "img = cv2.GaussianBlur(img,ksize=(3,3),sigmaX=2)\n",
    "\n",
    "# Find facial landmarks for each detected face\n",
    "face_locations = face_recognition.face_locations(img)\n",
    "#face_landmarks_list = face_recognition.face_landmarks(img)\n",
    "for (x, y, w, h) in face_locations:\n",
    "    landmarks = predictor(img,dlib.rectangle(left=x, top=y, right=w, bottom=h))\n",
    "    print(landmarks.parts())\n",
    "    landmarks_np = np.array([(landmark.x, landmark.y) for landmark in landmarks.parts()])\n",
    "    for (x, y) in landmarks_np:\n",
    "        cv2.circle(img,(x,y),2,(0,255,0),-1)\n",
    "        \n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Turn off axis labels and ticks\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     for n in range(0, 81):\n",
    "#         x1= landmarks.part(n).x\n",
    "#         y1= landmarks.part(n).y\n",
    "#         cv2.circle(img, (x1, y1), 2, (100, 105, 0), -1)\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')  # Turn off axis labels and ticks\n",
    "# plt.show()\n",
    "#         # Ensure coordinates (x, y) are within valid image bounds\n",
    "#         x = max(0, min(x, img.shape[1] - 1))\n",
    "#         y = max(0, min(y, img.shape[0] - 1))\n",
    "\n",
    "        #landmark = img[y,x]\n",
    "        \n",
    "# # Draw landmarks on the image\n",
    "# for landmarks in face_landmarks_list:\n",
    "#     for landmark_type, landmark_coords in landmarks.items():\n",
    "#         if landmark_type in additional_landmarks:\n",
    "#             for (x, y) in landmark_coords:\n",
    "#                 cv2.circle(image_rgb, (x, y), 2, (100, 105, 0), -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "load_labels_and_landmarks(train_dir_affect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Landmarks_Extraction from images of size (48,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-12T12:48:51.953034Z",
     "iopub.status.idle": "2023-09-12T12:48:51.953402Z",
     "shell.execute_reply": "2023-09-12T12:48:51.953250Z",
     "shell.execute_reply.started": "2023-09-12T12:48:51.953232Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to load labels and landmarks\n",
    "def load_labels_and_landmarks(directory):\n",
    "    \n",
    "    for expression_index, expression_dir in enumerate(os.listdir(directory)):\n",
    "        expression_path = os.path.join(directory, expression_dir)\n",
    "        if os.path.isdir(expression_path):\n",
    "            path=expression_path\n",
    "            parts = path.split(\"\\\\\")\n",
    "            #print(\"Expression:\", parts[-1])\n",
    "            for image_file in os.listdir(expression_path):\n",
    "                image_path = os.path.join(expression_path, image_file)\n",
    "                data = {\n",
    "                'Jawline_Landmark': [],\n",
    "                'Left_Eyebrow_Landmark': [],\n",
    "                'Right_Eyebrow_Landmark': [],\n",
    "                'Nose_Landmark': [],\n",
    "                'Left_Eye_Landmark': [],\n",
    "                'Right_Eye_Landmark': [],\n",
    "                'Mouth_Landmark': [],\n",
    "                'Forehead_Landmark': [],\n",
    "                'Label': []\n",
    "                }\n",
    "                img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                # Apply Gaussian blur\n",
    "                img = cv2.GaussianBlur(img,kernel_size=(2,2),sigma=2)\n",
    "                face_locations = face_recognition.face_locations(img)\n",
    "                face_landmarks_list = face_recognition.face_landmarks(img)\n",
    "                \n",
    "                for (x, y, w, h) in face_locations:\n",
    "                    landmarks = predictor(img,dlib.rectangle(left=x, top=y, right=w, bottom=h))\n",
    "                    jawline_landmarks,left_eyebrow_landmarks,right_eyebrow_landmarks,nose_landmarks,left_eye_landmarks,right_eye_landmarks, mouth_landmarks, forehead_landmarks =[],[],[],[],[],[],[],[]\n",
    "                    \n",
    "                    for n in range(0, 81):\n",
    "                        x = landmarks.part(n).x\n",
    "                        y = landmarks.part(n).y\n",
    "                        \n",
    "                        # Ensure coordinates (x, y) are within valid image bounds\n",
    "                        x = max(0, min(x, img.shape[1] - 1))\n",
    "                        y = max(0, min(y, img.shape[0] - 1))\n",
    "                        \n",
    "                        landmark = img[y,x]\n",
    "\n",
    "                        if 0 <= n <= 17:\n",
    "                            jawline_landmarks.append(landmark)\n",
    "                        elif 17 <= n <= 21:\n",
    "                            left_eyebrow_landmarks.append(landmark)\n",
    "                        elif 21 <= n <= 27:\n",
    "                            right_eyebrow_landmarks.append(landmark)\n",
    "                        elif 27 <= n <= 36:\n",
    "                            nose_landmarks.append(landmark)\n",
    "                        elif 36 <= n <= 42:\n",
    "                            left_eye_landmarks.append(landmark)\n",
    "                        elif 42 <= n <= 48:\n",
    "                            right_eye_landmarks.append(landmark)\n",
    "                        elif 48 <= n <= 68:\n",
    "                            mouth_landmarks.append(landmark)\n",
    "                        elif 68 <= n <= 81:\n",
    "                            forehead_landmarks.append(landmark)\n",
    "\n",
    "                    # Append landmarks to the data dictionary as lists\n",
    "                    data['Label'].append(expression_index)\n",
    "                    data['Jawline_Landmark'].append(jawline_landmarks)\n",
    "                    data['Left_Eyebrow_Landmark'].append(left_eyebrow_landmarks)\n",
    "                    data['Right_Eyebrow_Landmark'].append(right_eyebrow_landmarks)\n",
    "                    data['Nose_Landmark'].append(nose_landmarks)\n",
    "                    data['Left_Eye_Landmark'].append(left_eye_landmarks)\n",
    "                    data['Right_Eye_Landmark'].append(right_eye_landmarks)\n",
    "                    data['Mouth_Landmark'].append(mouth_landmarks)\n",
    "                    data['Forehead_Landmark'].append(forehead_landmarks)\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-12T12:48:51.954662Z",
     "iopub.status.idle": "2023-09-12T12:48:51.955530Z",
     "shell.execute_reply": "2023-09-12T12:48:51.955295Z",
     "shell.execute_reply.started": "2023-09-12T12:48:51.955267Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "features=load_labels_and_landmarks(train_dir_affect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Jawline_Landmark'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-12T12:48:51.993655Z",
     "iopub.status.idle": "2023-09-12T12:48:51.994137Z",
     "shell.execute_reply": "2023-09-12T12:48:51.993910Z",
     "shell.execute_reply.started": "2023-09-12T12:48:51.993885Z"
    },
    "id": "cTGYM9KrW3H1"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ExpressionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ExpressionClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)  # Batch normalization after the first convolution\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)  # Batch normalization after the second convolution\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)  # Batch normalization after the third convolution\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.Linear(128 * 46 * 46, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=1)\n",
    "        self.relu_deconv1 = nn.ReLU()\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=1)\n",
    "        self.relu_deconv2 = nn.ReLU()\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 1, kernel_size=2, stride=1)\n",
    "        self.relu_deconv3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)  # Apply batch normalization\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Apply batch normalization\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Apply batch normalization\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = ExpressionClassifier(num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"facial_landmarks.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-12T12:48:51.957079Z",
     "iopub.status.idle": "2023-09-12T12:48:51.957863Z",
     "shell.execute_reply": "2023-09-12T12:48:51.957637Z",
     "shell.execute_reply.started": "2023-09-12T12:48:51.957607Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming your features are loaded into NumPy arrays\n",
    "labels = labels  # Replace with your actual labels array\n",
    "features = {\n",
    "    'Left_Eye': left_eye_landmarks,\n",
    "    'Right_Eye': right_eye_landmarks,\n",
    "    'Nose': nose_landmarks,\n",
    "    'Mouth': mouth_landmarks,\n",
    "    'Jawline': jawline_landmarks,\n",
    "    'Forehead': forehead_landmarks,\n",
    "    'Left_Eyebrow': left_eyebrow_landmarks,\n",
    "    'Right_Eyebrow': right_eyebrow_landmarks    \n",
    "}\n",
    "\n",
    "# Calculate the length of each feature\n",
    "feature_lengths = {feature:len(array) for feature,array in features.items()}\n",
    "\n",
    "# Calculate the number of empty arrays (arrays with only zeros or NaN values) for each feature\n",
    "empty_arrays_count = {feature:np.count_nonzero(array == 0) for feature, array in features.items()}\n",
    "\n",
    "# Print the lengths of each feature and the number of empty arrays\n",
    "for feature, length in feature_lengths.items():\n",
    "    print(f'Length of {feature}: {length}')\n",
    "\n",
    "for feature, count in empty_arrays_count.items():\n",
    "    print(f'Number of empty arrays in {feature}: {count}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check if all arrays have the same shape or not\n",
    "for feature in features:\n",
    "    first_shape=features[feature][0].shape\n",
    "    all_same_shape = all(arr.shape == first_shape for arr in features[feature])\n",
    "    if all_same_shape:\n",
    "        print(\"{} all arrays have the same shape.\".format(feature))\n",
    "    else:\n",
    "        print(\"{} arrays have different shapes.\".format(feature))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(jawline_landmarks)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def find_maximum_shape(feature):\n",
    "    for arr in features[feature]:\n",
    "        if isinstance(arr, np.ndarray):\n",
    "            return arr.shape\n",
    "        elif isinstance(arr, list):\n",
    "            # If arr is a list, recursively find the maximum shape among its elements\n",
    "            shapes = [find_maximum_shape(sub_arr) for sub_arr in arr]\n",
    "            # Find the maximum shape among the elements\n",
    "            max_shape = tuple(max(dim) for dim in zip(*shapes))\n",
    "            return max_shape\n",
    "        else:\n",
    "            # Handle other data types (e.g., scalars)\n",
    "            return ()\n",
    "\n",
    "max_shape = find_maximum_shape(\"Left_Eye\")\n",
    "\n",
    "# Print the maximum shape\n",
    "print(\"Maximum Shape:\", max_shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for feature in features.keys():\n",
    "    max_shape = find_maximum_shape(feature)\n",
    "    # Print the maximum shape\n",
    "    print(\"Maximum Shape of {}:\".format(feature),max_shape)\n",
    "    print(\"before reshaping of {}:\".format(feature),len(features[feature]))#before reshaping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for feature in features.keys():\n",
    "    if feature == 'Forehead':\n",
    "        continue\n",
    "    max_shape = find_maximum_shape(feature)\n",
    "    # Print the maximum shape\n",
    "    print(\"Maximum Shape of {}:\".format(feature),max_shape)\n",
    "    print(\"before reshaping of {}:\".format(feature),len(features[feature]))#before reshaping\n",
    "    # Resize or pad all arrays within the feature to the common shape\n",
    "    feature_arrays = []\n",
    "    for arr in features[feature]:\n",
    "        if isinstance(arr, np.ndarray):\n",
    "            if arr.shape!= max_shape:\n",
    "                if arr.shape[0]<max_shape[0] or arr.shape[1]<max_shape[1]:\n",
    "                    # Pad the array with zeros\n",
    "                    padded_arr=np.zeros(max_shape,dtype=arr.dtype)\n",
    "                    padded_arr[:arr.shape[0],:arr.shape[1]]=arr\n",
    "                    feature_arrays.append(padded_arr)\n",
    "                else:\n",
    "                    # Resize the array using interpolation (e.g., bilinear)\n",
    "                    resized_arr = cv2.resize(arr,(max_shape[1], max_shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "                    feature_arrays.append(resized_arr)\n",
    "            else:\n",
    "                feature_arrays.append(arr)\n",
    "\n",
    "    # Replace the original list of arrays in the dictionary with the resized/padded arrays\n",
    "    features[feature] = feature_arrays\n",
    "\n",
    "# Print the modified dictionary\n",
    "# print(features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for feature in features.keys():\n",
    "    print(\"After reshaping of {} landmarks:\".format(feature),len(features[feature]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check if all arrays have the same shape or not\n",
    "for feature in features:\n",
    "    first_shape=features[feature][0].shape\n",
    "    all_same_shape = all(arr.shape == first_shape for arr in features[feature])\n",
    "    if all_same_shape:\n",
    "        print(\"{} all arrays have the same shape.\".format(feature))\n",
    "    else:\n",
    "        print(\"{} arrays have different shapes.\".format(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=list(features.keys())\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-12T12:48:51.967982Z",
     "iopub.status.idle": "2023-09-12T12:48:51.968905Z",
     "shell.execute_reply": "2023-09-12T12:48:51.968677Z",
     "shell.execute_reply.started": "2023-09-12T12:48:51.968648Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # Create a DataFrame from the dictionary\n",
    "# df = pd.DataFrame(columns=column_names)\n",
    "# for key, value in features.items():\n",
    "#     df[key] = df[key].append(value,ignore_index=False)\n",
    "# # # Find the maximum length of arrays in each column\n",
    "# # max_len = df.applymap(lambda x: len(x) if isinstance(x, list) else 0).max()\n",
    "\n",
    "# # # Fill missing values with NaN to make all columns have the same length\n",
    "# # df = df.apply(lambda col: col + [torch.tensor(np.nan, dtype=torch.float32)] * (max_len - len(col)))\n",
    "\n",
    "# # # Display the DataFrame\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2023-09-12T12:48:51.986730Z",
     "iopub.status.idle": "2023-09-12T12:48:51.987227Z",
     "shell.execute_reply": "2023-09-12T12:48:51.986980Z",
     "shell.execute_reply.started": "2023-09-12T12:48:51.986956Z"
    },
    "id": "0JKBe3MKQSIg",
    "outputId": "9a5ef499-52b4-47c5-bf8c-c8cea6d60be7"
   },
   "outputs": [],
   "source": [
    " # Load training images and labels\n",
    "\n",
    "# Load validation images and labels\n",
    "# validation_labels,validation_landmarks_list=load_labels_and_landmarks(validation_dir)\n",
    "\n",
    "# # # Normalize image data\n",
    "# train_images=train_images/255\n",
    "# validation_images=validation_images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-12T12:48:51.988575Z",
     "iopub.status.idle": "2023-09-12T12:48:51.989469Z",
     "shell.execute_reply": "2023-09-12T12:48:51.989247Z",
     "shell.execute_reply.started": "2023-09-12T12:48:51.989221Z"
    },
    "id": "nh5FczOFDHIY"
   },
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self,Left_Eye,Right_Eye,Nose,Mouth,Jawline,Forehead,Left_Eyebrow,Right_Eyebrow,labels,transform=None):\n",
    "#         self.images = images\n",
    "#         self.Left_Eye=Left_Eye\n",
    "#         self.Right_Eye=Right_Eye\n",
    "#         self.Nose=Nose\n",
    "#         self.Mouth=Mouth\n",
    "#         self.Jawline=Jawline\n",
    "#         self.Forehead=Forehead\n",
    "#         self.Left_Eyebrow=Left_Eyebrow\n",
    "#         self.Right_Eyebrow=Right_Eyebrow\n",
    "#         self.labels = labels\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = self.images[idx]\n",
    "#         label = self.labels[idx]\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "\n",
    "#         return image, label\n",
    "\n",
    "# data_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "# batch_size = 1024\n",
    "# train_dataset = CustomDataset(train_images, train_labels, transform=data_transform)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# validation_dataset = CustomDataset(validation_images, validation_labels, transform=data_transform)\n",
    "# validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-12T12:48:51.991323Z",
     "iopub.status.idle": "2023-09-12T12:48:51.991861Z",
     "shell.execute_reply": "2023-09-12T12:48:51.991614Z",
     "shell.execute_reply.started": "2023-09-12T12:48:51.991587Z"
    },
    "id": "cMnegsKVWmwb"
   },
   "outputs": [],
   "source": [
    "# Input_Size=48\n",
    "# Filter_Size=2\n",
    "# Pool_Size=2\n",
    "# Padding=1\n",
    "# Stride=1\n",
    "\n",
    "# for iteration in range(1, 5):\n",
    "#     print(f\"Hidden layer{iteration}\")\n",
    "#     Output_Size = (Input_Size-Filter_Size+2*Padding)/Stride + 1\n",
    "#     print(f\"convolutional layer{iteration}: Output size = {Output_Size}\")\n",
    "#     Output_Size = (Input_Size-Pool_Size)/Stride + 1\n",
    "#     print(f\"pooling layer{iteration}: Output size = {np.floor(Output_Size)}\")\n",
    "#     Input_Size = Output_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to tensors within the dictionary\n",
    "for key, value in features.items():\n",
    "    features[key] = [torch.tensor(arr, dtype=torch.float32) for arr in value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features['Left_Eye'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2023-09-12T12:48:51.995801Z",
     "iopub.status.idle": "2023-09-12T12:48:51.996297Z",
     "shell.execute_reply": "2023-09-12T12:48:51.996061Z",
     "shell.execute_reply.started": "2023-09-12T12:48:51.996037Z"
    },
    "id": "1ihzWs38Iduo",
    "outputId": "08abcae5-3cf6-4483-b5df-7565bf8c55ee"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.to(device, dtype=torch.float)\n",
    "plotlosses = PlotLossesKeras()\n",
    "\n",
    "for feature in fea\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images = images.to(device, dtype=torch.float)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {loss.item():.4f} Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvEPVZWoDHLa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEb72BcUDHOG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPi55mL1DHRj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
